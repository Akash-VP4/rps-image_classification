{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db3370f",
   "metadata": {},
   "source": [
    "## RPC- Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "import os\n",
    "import torch,torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b089d2",
   "metadata": {},
   "source": [
    "#### Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798cb49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class names: ['scissors', 'paper', 'rock']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2188 files [00:02, 867.76 files/s]\n"
     ]
    }
   ],
   "source": [
    "file_name = \"dataset\"\n",
    "\n",
    "class_names = os.listdir(file_name)\n",
    "print(f\"class names: {class_names}\")\n",
    "\n",
    "# Splitting folders\n",
    "splitfolders.ratio(file_name,seed=1337, ratio=(0.8,0.2))\n",
    "\n",
    "train_path = \"./output/train\"\n",
    "test_path = \"./output/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d0a05474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bititude/anaconda3/lib/python3.13/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = v2.Compose([\n",
    "  v2.Resize((64,64)),\n",
    "  v2.ToTensor(),\n",
    "  v2.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "32a49a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for training\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=train_path,transform=transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=test_path,transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed317291",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label=train_dataset[1234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "455a8d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7412, -0.7333, -0.7412,  ..., -0.7020, -0.7098, -0.7176],\n",
       "         [-0.7333, -0.7333, -0.7490,  ..., -0.7176, -0.7176, -0.7176],\n",
       "         [-0.7490, -0.7412, -0.7490,  ..., -0.7333, -0.7412, -0.7412],\n",
       "         ...,\n",
       "         [-0.7647, -0.7569, -0.7647,  ..., -0.7412, -0.7412, -0.7412],\n",
       "         [-0.7647, -0.7490, -0.7647,  ..., -0.7412, -0.7490, -0.7490],\n",
       "         [-0.7490, -0.7490, -0.7412,  ..., -0.7176, -0.7098, -0.7176]],\n",
       "\n",
       "        [[-0.0745, -0.0667, -0.0588,  ..., -0.0118, -0.0275, -0.0431],\n",
       "         [-0.0588, -0.0588, -0.0588,  ..., -0.0039, -0.0196, -0.0275],\n",
       "         [-0.0431, -0.0353, -0.0353,  ...,  0.0039, -0.0196, -0.0275],\n",
       "         ...,\n",
       "         [ 0.0039,  0.0118,  0.0118,  ...,  0.0196,  0.0039,  0.0039],\n",
       "         [-0.0118,  0.0118,  0.0118,  ...,  0.0118, -0.0039, -0.0118],\n",
       "         [-0.0275, -0.0118, -0.0039,  ..., -0.0039, -0.0196, -0.0275]],\n",
       "\n",
       "        [[-0.6784, -0.6784, -0.6784,  ..., -0.6784, -0.6863, -0.7020],\n",
       "         [-0.6706, -0.6706, -0.6784,  ..., -0.6784, -0.6863, -0.6941],\n",
       "         [-0.6784, -0.6706, -0.6627,  ..., -0.6863, -0.6941, -0.7020],\n",
       "         ...,\n",
       "         [-0.6627, -0.6392, -0.6471,  ..., -0.6784, -0.6784, -0.6863],\n",
       "         [-0.6627, -0.6392, -0.6392,  ..., -0.6706, -0.6941, -0.6863],\n",
       "         [-0.6784, -0.6549, -0.6471,  ..., -0.6627, -0.6706, -0.6863]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b8183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
